import numpy as np

from .evidence import Evidence
from .param_sampling import Sampler


class Walker:
    """
    A class that encapsulates the sampling configuration for a Bayesian
    inference problem, including the physical model and likelihood model
    configurations. It manages the sampling process for both the physical
    model and the likelihood model parameters, alternating between each
    model in a Gibbs sampling framework.
    """

    def __init__(
        self,
        model_sampler: Sampler,
        evidence: Evidence,
        likelihood_samplers: list[Sampler] = [],
        rng: np.random.Generator = np.random.default_rng(42),
    ):
        """
        Initialize the Sampler with a list of samplers.

        Parameters:
        ----------
        model_sampler: Sampler
            A Sampler object for physical model parameters.
        evidence: Evidence
            A Evidence object containing the data for which the likelihood
            model is evaluated.
        likelihood_model_samplers: list[Sampler]
            A list of Sampler objects for likelihood model parameters.
            Corresponds to the order of `evidence.parametric_constraints`.
        rng: np.random.Generator, optional
            A random number generator for reproducibility. Defaults to a new
            default_rng with a fixed seed.
        """

        # constant attributes
        self.model_sampler = model_sampler
        self.likelihood_samplers = likelihood_samplers
        self.evidence = evidence
        self.rng = rng

        self.gibbs_sampling = len(self.likelihood_samplers) > 0

        if self.evidence.model_params != self.model_sampler.params:
            raise ValueError(
                "Inconsistent physical model parameters between "
                "'evidence' and 'model_sampler'"
            )

        if len(self.likelihood_samplers) != len(self.evidence.parametric_constraints):
            raise ValueError(
                "The lists 'likelihood_samplers' and "
                "'evidence.parametric_constraints' must correspond!"
            )
        for i, conf in enumerate(self.likelihood_samplers):
            constraint = self.evidence.parametric_constraints[i]
            if constraint.likelihood.params != conf.params:
                raise ValueError(
                    "Inconsistent likelihood model parameters"
                    f"between 'likelihood_samplers[{i}]' and "
                    f"'evidence.parametric_constraints[{i}]'"
                )

    def run_model_batch(self, n_steps, x0, likelihood_params=[], burn=False):
        """
        Walks the model parameter space for fixed values of the
        `likelihood_params`

        Parameters:
        ----------
        n_steps: int
            The number of steps to run for the model sampling.
        x0: np.ndarray
            The starting location for the model sampling.
        likelihood_params: list[tuple]
            A list of fixed values of the parameters for each of the
            parametric likelihood models, corresponding to the order
            of `self.likelihood_samplers`
        burn: bool
            If True, the batch is considered a burn-in batch and
            the acceptance rate, log probabilities, and parameter
            chain will not be recorded.

        Returns:
        -------
        batch_chain: np.ndarray
            The parameter chain generated by the model sampling algorithm.
        logp: np.ndarray
            The log probabilities of the parameter chain.
        accepted: float
            The acceptance rate of the model sampling algorithm.
        """
        self.model_sampler.sample(
            n_steps,
            x0,
            self.rng,
            lambda x: self.log_posterior(x, likelihood_params),
            burn=burn,
        )

    def run_likelihood_batches(
        self, n_steps, starting_locations, model_params, burn=False
    ):
        """
        Walks each of the likelihood parameter spacers one by one, for a
        fixed value of `model_params`

        Parameters:
        ----------
        n_steps: int
            The number of steps to run for each likelihood model.
        starting_locations: list[np.ndarray]
            A list of starting locations for each likelihood model.
        model_params: tuple
            A fixed value of the parameters for the physical model
        burn: bool
            If True, the batch is considered a burn-in batch and
            the acceptance rate, log probabilities, and parameter
            chain will not be recorded.

        Returns:
        -------
        list[np.ndarray]
            A list of parameter chains for each likelihood model.
        logp : list[np.ndarray]
            A list of log probabilities for each likelihood model.
        accepted : list[float]
            A list of acceptance rates for each likelihood model.
        """
        for i, sampler in enumerate(self.likelihood_samplers):
            constraint = self.evidence.parametric_constraints[i]

            # precompute the model prediction for just this constraint,
            # as the physical model parameters (and thus the physical
            # model prediction) will be fixed for this walk
            ym = constraint.predict(*model_params)

            # the posterior pdf of this walk only accounts for the
            # observables in the constraint containing the likelihood
            # model whose parameters we're sampling, rather than the
            # whole evidence
            def log_posterior_lm(x):
                return sampler.prior.logpdf(x) + constraint.marginal_log_likelihood(
                    ym, x
                )

            # get the starting location for this likelihood model
            x0 = starting_locations[i]

            # run a chain over the parameter space of just this
            # likelihood model
            sampler.sample(n_steps, x0, self.rng, log_posterior_lm, burn=burn)

    def log_likelihood(self, model_params, likelihood_params):
        return self.evidence.log_likelihood(model_params, likelihood_params)

    def log_posterior(self, model_params, likelihood_params):
        return self.log_likelihood(model_params, likelihood_params) + self.log_prior(
            model_params, likelihood_params
        )

    def log_prior(self, model_params, likelihood_params):
        """
        Returns the log-prior probability of the model parameters and
        likelihood parameters.

        Parameters:
        ----------
        model_params: tuple
            The parameters of the physical model.
        likelihood_params: list[tuple]
            A list of tuples containing additional parameters for the
            likelihood model for each constraint.

        Returns:
        -------
        float
            The log-prior probability.
        """
        return self.model_sampler.prior.logpdf(model_params) + sum(
            lm.prior.logpdf(likelihood_params[i])
            for i, lm in enumerate(self.likelihood_samplers)
        )

    def walk(
        self,
        n_steps: int,
        burnin: int = 0,
        batch_size: int = None,
        verbose: bool = True,
    ):
        """
        Runs the MCMC chain with the specified parameters.
        Updates the internal state of the `model_sampler` and
        `likelihood_samplers` with records of the walk and relevant
        statistics.

        Parameters:
        -----------
            n_steps : int
                Total number of active steps for the MCMC chain.
            batch_size : int
                Number of steps per batch.
            burnin : int
                Number of extra burn-in steps to do before active steps
            verbose : bool
                Flag to print extra logging information.
        """
        if batch_size is not None:
            rem_burn = burnin % batch_size
            n_burn_batches = burnin // batch_size
            burn_batches = n_burn_batches * [batch_size] + (rem_burn > 0) * [rem_burn]

            rem = n_steps % batch_size
            n_full_batches = n_steps // batch_size
            batches = n_full_batches * [batch_size] + (rem > 0) * [rem]
        else:
            batches = [n_steps]
            burn_batches = [burnin]

        if burnin == 0:
            burn_batches = []

        # burn in
        for i, steps_in_batch in enumerate(burn_batches):
            # Gibb's sample physical model parameters
            self.run_model_batch(
                steps_in_batch,
                self.model_sampler.state,
                [sampler.state for sampler in self.likelihood_samplers],
                burn=True,
            )

            if self.gibbs_sampling:
                # Gibb's sample likelihood model parameters
                self.run_likelihood_batches(
                    steps_in_batch,
                    [sampler.state for sampler in self.likelihood_samplers],
                    self.model_sampler.state,
                    burn=True,
                )

            if verbose:
                print(
                    f"Burn-in batch {i + 1}/{len(burn_batches)}"
                    f" completed, {steps_in_batch} steps."
                )

        # do real walk
        for i, steps_in_batch in enumerate(batches):

            # Gibb's sample physical model parameters
            self.run_model_batch(
                steps_in_batch,
                self.model_sampler.state,
                [sampler.state for sampler in self.likelihood_samplers],
            )

            if self.gibbs_sampling:
                # Gibb's sample likelihood model parameters
                self.run_likelihood_batches(
                    steps_in_batch,
                    [sampler.state for sampler in self.likelihood_samplers],
                    self.model_sampler.state,
                )

            if verbose:
                msg = (
                    f"Batch: {i + 1}/{len(batches)} completed, "
                    f"{steps_in_batch} steps. "
                    f"\n  Model parameter acceptance fraction: "
                    f"{self.model_sampler.most_recent_batch_acceptance_fraction():.3f}"
                )
                if self.gibbs_sampling:
                    msg += (
                        f"\n  Likelihood parameter acceptance fractions: "
                        f"{[sampler.most_recent_batch_acceptance_fraction() for sampler in self.likelihood_samplers]}"
                    )
                print(msg)
